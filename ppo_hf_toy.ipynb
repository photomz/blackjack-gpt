{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eZge8FxefWv",
        "outputId": "c6d1a71e-c7a6-4978-d322-4a84b5291f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "# Adapted from: https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py\n",
        "# prompt: Convert to pip installs: from dataclasses import dataclass, field from typing import Optional  import torch import tyro from accelerate import Accelerator from datasets import load_dataset from peft import LoraConfig from tqdm import tqdm from transformers import AutoTokenizer, pipeline  from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed from trl.core import LengthSampler from trl.import_utils import is_npu_available, is_xpu_available\n",
        "\n",
        "!pip install -q torch tyro accelerate datasets peft tqdm transformers trl wandb\n",
        "!wandb login API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "p9VXaTv5ebj8",
        "outputId": "ea79d6b3-d144-4eba-f026-e8830f0c5e70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphotonmz\u001b[0m (\u001b[33msoftermax\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240114_020328-zjcfzwr0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/softermax/trl/runs/zjcfzwr0' target=\"_blank\">golden-sun-1</a></strong> to <a href='https://wandb.ai/softermax/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/softermax/trl' target=\"_blank\">https://wandb.ai/softermax/trl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/softermax/trl/runs/zjcfzwr0' target=\"_blank\">https://wandb.ai/softermax/trl/runs/zjcfzwr0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n",
            "5it [00:15,  3.00s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "2029it [1:39:34,  2.89s/it]/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1212: UserWarning: The average ratio of batch (26.61) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "2070it [1:41:33,  2.87s/it]/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1212: UserWarning: The average ratio of batch (1638.41) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1212: UserWarning: The average ratio of batch (21955.46) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "2483it [2:01:32,  2.73s/it]/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1212: UserWarning: The average ratio of batch (17.32) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1212: UserWarning: The average ratio of batch (170.84) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "3011it [2:26:50,  2.88s/it]/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:1212: UserWarning: The average ratio of batch (24.11) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "3111it [2:31:45,  2.93s/it]\n"
          ]
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import tyro\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "from trl import (\n",
        "    AutoModelForCausalLMWithValueHead,\n",
        "    AutoModelForSeq2SeqLMWithValueHead,\n",
        "    PPOConfig,\n",
        "    PPOTrainer,\n",
        "    set_seed,\n",
        ")\n",
        "from trl.core import LengthSampler\n",
        "from trl.import_utils import is_npu_available, is_xpu_available\n",
        "\n",
        "\n",
        "tqdm.pandas()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    ppo_config: PPOConfig = field(\n",
        "        default_factory=lambda: PPOConfig(\n",
        "            model_name=\"lvwerra/gpt2-imdb\",\n",
        "            query_dataset=\"imdb\",\n",
        "            reward_model=\"sentiment-analysis:lvwerra/distilbert-imdb\",\n",
        "            learning_rate=1.41e-5,\n",
        "            log_with=\"wandb\",\n",
        "            mini_batch_size=8,\n",
        "            batch_size=8,\n",
        "            gradient_accumulation_steps=1,\n",
        "            early_stopping=False,\n",
        "            target_kl=6.0,\n",
        "            kl_penalty=\"kl\",\n",
        "            seed=0,\n",
        "            use_score_scaling=False,\n",
        "            use_score_norm=False,\n",
        "            score_clip=None,\n",
        "        )\n",
        "    )\n",
        "    use_seq2seq: bool = False\n",
        "    \"\"\"whether to use seq2seq models\"\"\"\n",
        "    use_peft: bool = False\n",
        "    \"\"\"whether to use peft\"\"\"\n",
        "    peft_config: Optional[LoraConfig] = field(\n",
        "        default_factory=lambda: LoraConfig(\n",
        "            r=16,\n",
        "            lora_alpha=16,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        ),\n",
        "    )\n",
        "    trust_remote_code: bool = field(\n",
        "        default=False, metadata={\"help\": \"Enable `trust_remote_code`\"}\n",
        "    )\n",
        "\n",
        "\n",
        "args = ScriptArguments()  # tyro.cli(ScriptArguments) # ignore args on Colab\n",
        "\n",
        "\n",
        "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
        "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
        "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
        "\n",
        "trl_model_class = (\n",
        "    AutoModelForCausalLMWithValueHead\n",
        "    if not args.use_seq2seq\n",
        "    else AutoModelForSeq2SeqLMWithValueHead\n",
        ")\n",
        "\n",
        "\n",
        "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
        "# from the `datasets` library. One should customize this function to train the model on\n",
        "# its own dataset.\n",
        "def build_dataset(\n",
        "    config, query_dataset, input_min_text_length=2, input_max_text_length=8\n",
        "):\n",
        "    \"\"\"\n",
        "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
        "    customize this function to train the model on its own dataset.\n",
        "\n",
        "    Args:\n",
        "        query_dataset (`str`):\n",
        "            The name of the dataset to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dataloader (`torch.utils.data.DataLoader`):\n",
        "            The dataloader for the dataset.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # load imdb with datasets\n",
        "    ds = load_dataset(query_dataset, split=\"train\")\n",
        "    ds = ds.rename_columns({\"text\": \"review\"})\n",
        "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
        "\n",
        "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds\n",
        "\n",
        "\n",
        "# We retrieve the dataloader by calling the `build_dataset` function.\n",
        "dataset = build_dataset(args.ppo_config, args.ppo_config.query_dataset)\n",
        "\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "\n",
        "# set seed before initializing value head for deterministic eval\n",
        "set_seed(args.ppo_config.seed)\n",
        "\n",
        "# Now let's build the model, the reference model, and the tokenizer.\n",
        "if not args.use_peft:\n",
        "    ref_model = trl_model_class.from_pretrained(\n",
        "        args.ppo_config.model_name, trust_remote_code=args.trust_remote_code\n",
        "    )\n",
        "    device_map = None\n",
        "    peft_config = None\n",
        "else:\n",
        "    peft_config = args.peft_config\n",
        "    ref_model = None\n",
        "    # Copy the model to each device\n",
        "    device_map = {\"\": Accelerator().local_process_index}\n",
        "\n",
        "model = trl_model_class.from_pretrained(\n",
        "    args.ppo_config.model_name,\n",
        "    trust_remote_code=args.trust_remote_code,\n",
        "    device_map=device_map,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)\n",
        "\n",
        "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    args.ppo_config,\n",
        "    model,\n",
        "    ref_model,\n",
        "    tokenizer,\n",
        "    dataset=dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "# We then build the sentiment analysis pipeline, passing the model name and the\n",
        "# sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
        "# to the same device as the PPOTrainer.\n",
        "device = ppo_trainer.accelerator.device\n",
        "if ppo_trainer.accelerator.num_processes == 1:\n",
        "    if is_xpu_available():\n",
        "        device = \"xpu:0\"\n",
        "    elif is_npu_available():\n",
        "        device = \"npu:0\"\n",
        "    else:\n",
        "        device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
        "ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin\n",
        "task, model_name = args.ppo_config.reward_model.split(\":\")\n",
        "if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():\n",
        "    with ds_plugin.zero3_init_context_manager(enable=False):\n",
        "        sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
        "else:\n",
        "    sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
        "\n",
        "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
        "if sentiment_pipe.tokenizer.pad_token_id is None:\n",
        "    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "if sentiment_pipe.model.config.pad_token_id is None:\n",
        "    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# We then define the arguments to pass to the `generate` function. These arguments\n",
        "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
        "# the `generate` function of the trained model.\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"max_new_tokens\": 32,\n",
        "}\n",
        "\n",
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Get response from gpt2\n",
        "    response_tensors, ref_response_tensors = ppo_trainer.generate(\n",
        "        query_tensors,\n",
        "        return_prompt=False,\n",
        "        generate_ref_response=True,\n",
        "        **generation_kwargs\n",
        "    )\n",
        "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
        "    batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)\n",
        "\n",
        "    # Compute sentiment score\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
        "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
        "    ref_texts = [q + r for q, r in zip(batch[\"query\"], batch[\"ref_response\"])]\n",
        "    ref_pipe_outputs = sentiment_pipe(ref_texts, **sent_kwargs)\n",
        "    ref_rewards = [torch.tensor(output[1][\"score\"]) for output in ref_pipe_outputs]\n",
        "    batch[\"ref_rewards\"] = ref_rewards\n",
        "\n",
        "    # Run PPO step\n",
        "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "    ppo_trainer.log_stats(\n",
        "        stats,\n",
        "        batch,\n",
        "        rewards,\n",
        "        columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rktj3qrOsLr_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
